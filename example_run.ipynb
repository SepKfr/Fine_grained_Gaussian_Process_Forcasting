{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsxE/8Ur6yl/aMuTchApoC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SepKfr/Corruption-resilient-Forecasting-Models/blob/master/example_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SepKfr/Corruption-resilient-Forecasting-Models.git"
      ],
      "metadata": {
        "id": "D6QxdQA1V_Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Corruption-resilient-Forecasting-Models/"
      ],
      "metadata": {
        "id": "ePk50qvuWGyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash run.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYMnjlSdpWqy",
        "outputId": "7263984d-2b81-47cf-b9b4-8345a79b72c0"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 22:20:09,061]\u001b[0m A new study created in memory with name: ATA_gp\u001b[0m\n",
            "Train epoch: 0, loss: 124.6345\n",
            "val loss: 14.5438\n",
            "Train epoch: 5, loss: 102.1675\n",
            "val loss: 11.9817\n",
            "Train epoch: 10, loss: 79.0824\n",
            "val loss: 9.0511\n",
            "Train epoch: 15, loss: 71.3089\n",
            "val loss: 8.3995\n",
            "Train epoch: 20, loss: 65.9213\n",
            "val loss: 7.9061\n",
            "/usr/local/lib/python3.8/dist-packages/gpytorch/distributions/multivariate_normal.py:319: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
            "  warnings.warn(\n",
            "Train epoch: 25, loss: 61.2346\n",
            "val loss: 7.5152\n",
            "Train epoch: 30, loss: 57.6559\n",
            "val loss: 7.2874\n",
            "Train epoch: 35, loss: 54.7390\n",
            "val loss: 7.1164\n",
            "Train epoch: 40, loss: 52.9050\n",
            "val loss: 7.0363\n",
            "Train epoch: 45, loss: 51.0077\n",
            "val loss: 6.6284\n",
            "\u001b[32m[I 2023-01-30 22:26:13,714]\u001b[0m Trial 0 finished with value: 6.523269236087799 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.523269236087799.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 22:26:13,718]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 130.1344\n",
            "val loss: 14.9043\n",
            "Train epoch: 5, loss: 101.5578\n",
            "val loss: 11.8888\n",
            "Train epoch: 10, loss: 80.1064\n",
            "val loss: 10.9485\n",
            "Train epoch: 15, loss: 73.7002\n",
            "val loss: 9.0598\n",
            "Train epoch: 20, loss: 70.2616\n",
            "val loss: 8.2430\n",
            "Train epoch: 25, loss: 67.3733\n",
            "val loss: 8.1363\n",
            "Train epoch: 30, loss: 64.6190\n",
            "val loss: 7.8215\n",
            "Train epoch: 35, loss: 62.8582\n",
            "val loss: 7.8243\n",
            "Train epoch: 40, loss: 60.8965\n",
            "val loss: 7.4588\n",
            "Train epoch: 45, loss: 60.2120\n",
            "val loss: 7.3835\n",
            "\u001b[32m[I 2023-01-30 22:31:35,009]\u001b[0m Trial 2 finished with value: 7.1747225522994995 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.523269236087799.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  6.523269236087799\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.5303\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 22:32:55,132]\u001b[0m A new study created in memory with name: ATA_iso\u001b[0m\n",
            "Train epoch: 0, loss: 130.0025\n",
            "val loss: 14.8539\n",
            "Train epoch: 5, loss: 102.6096\n",
            "val loss: 12.6807\n",
            "Train epoch: 10, loss: 89.6489\n",
            "val loss: 10.6897\n",
            "Train epoch: 15, loss: 75.4009\n",
            "val loss: 8.7167\n",
            "Train epoch: 20, loss: 68.3223\n",
            "val loss: 8.0555\n",
            "/usr/local/lib/python3.8/dist-packages/gpytorch/distributions/multivariate_normal.py:319: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
            "  warnings.warn(\n",
            "Train epoch: 25, loss: 64.1758\n",
            "val loss: 7.7233\n",
            "Train epoch: 30, loss: 60.9945\n",
            "val loss: 7.4002\n",
            "Train epoch: 35, loss: 58.5863\n",
            "val loss: 7.7480\n",
            "Train epoch: 40, loss: 56.3226\n",
            "val loss: 7.2611\n",
            "Train epoch: 45, loss: 54.6235\n",
            "val loss: 6.9251\n",
            "\u001b[32m[I 2023-01-30 22:38:35,278]\u001b[0m Trial 0 finished with value: 6.872766315937042 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.872766315937042.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 22:38:35,283]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 136.5574\n",
            "val loss: 14.9858\n",
            "Train epoch: 5, loss: 102.7857\n",
            "val loss: 12.0973\n",
            "Train epoch: 10, loss: 82.1855\n",
            "val loss: 10.2313\n",
            "Train epoch: 15, loss: 76.3692\n",
            "val loss: 9.5774\n",
            "Train epoch: 20, loss: 71.7608\n",
            "val loss: 8.8545\n",
            "Train epoch: 25, loss: 68.4114\n",
            "val loss: 8.3250\n",
            "Train epoch: 30, loss: 65.9383\n",
            "val loss: 8.0414\n",
            "Train epoch: 35, loss: 64.0695\n",
            "val loss: 8.0816\n",
            "Train epoch: 40, loss: 62.7517\n",
            "val loss: 7.6342\n",
            "Train epoch: 45, loss: 60.9935\n",
            "val loss: 7.5369\n",
            "\u001b[32m[I 2023-01-30 22:43:48,707]\u001b[0m Trial 2 finished with value: 7.380939394235611 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.872766315937042.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  6.872766315937042\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.5587\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 22:45:09,173]\u001b[0m A new study created in memory with name: ATA_no\u001b[0m\n",
            "Train epoch: 0, loss: 129.5115\n",
            "val loss: 14.8025\n",
            "Train epoch: 5, loss: 110.1250\n",
            "val loss: 12.9482\n",
            "Train epoch: 10, loss: 88.1993\n",
            "val loss: 9.8879\n",
            "Train epoch: 15, loss: 77.4390\n",
            "val loss: 8.8853\n",
            "Train epoch: 20, loss: 71.1677\n",
            "val loss: 8.5790\n",
            "Train epoch: 25, loss: 67.5026\n",
            "val loss: 8.2633\n",
            "Train epoch: 30, loss: 65.2506\n",
            "val loss: 8.0876\n",
            "Train epoch: 35, loss: 63.1252\n",
            "val loss: 8.0946\n",
            "Train epoch: 40, loss: 61.5801\n",
            "val loss: 7.8372\n",
            "Train epoch: 45, loss: 60.3763\n",
            "val loss: 7.7166\n",
            "\u001b[32m[I 2023-01-30 22:50:07,256]\u001b[0m Trial 0 finished with value: 7.671910643577576 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 7.671910643577576.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 22:50:07,259]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 136.2604\n",
            "val loss: 14.9406\n",
            "Train epoch: 5, loss: 109.8606\n",
            "val loss: 12.9811\n",
            "Train epoch: 10, loss: 83.7822\n",
            "val loss: 10.0308\n",
            "Train epoch: 15, loss: 78.9630\n",
            "val loss: 9.6636\n",
            "Train epoch: 20, loss: 73.8739\n",
            "val loss: 8.9766\n",
            "Train epoch: 25, loss: 70.2281\n",
            "val loss: 8.3854\n",
            "Train epoch: 30, loss: 68.0895\n",
            "val loss: 8.3684\n",
            "Train epoch: 35, loss: 66.0952\n",
            "val loss: 8.2154\n",
            "Train epoch: 40, loss: 64.0385\n",
            "val loss: 8.0596\n",
            "Train epoch: 45, loss: 62.8386\n",
            "val loss: 7.8681\n",
            "\u001b[32m[I 2023-01-30 22:54:46,809]\u001b[0m Trial 2 finished with value: 7.82660984992981 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 7.671910643577576.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  7.671910643577576\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.6288\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 22:56:08,048]\u001b[0m A new study created in memory with name: ATA_gp\u001b[0m\n",
            "Train epoch: 0, loss: 128.3002\n",
            "val loss: 14.7192\n",
            "Train epoch: 5, loss: 103.6078\n",
            "val loss: 12.5703\n",
            "Train epoch: 10, loss: 81.9940\n",
            "val loss: 9.7657\n",
            "Train epoch: 15, loss: 72.4510\n",
            "val loss: 8.5570\n",
            "Train epoch: 20, loss: 67.4455\n",
            "val loss: 7.9873\n",
            "/usr/local/lib/python3.8/dist-packages/gpytorch/distributions/multivariate_normal.py:319: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
            "  warnings.warn(\n",
            "Train epoch: 25, loss: 63.3623\n",
            "val loss: 7.7395\n",
            "Train epoch: 30, loss: 59.8929\n",
            "val loss: 7.5416\n",
            "Train epoch: 35, loss: 57.6036\n",
            "val loss: 7.0835\n",
            "Train epoch: 40, loss: 55.3312\n",
            "val loss: 7.3504\n",
            "Train epoch: 45, loss: 52.7531\n",
            "val loss: 7.0210\n",
            "\u001b[32m[I 2023-01-30 23:01:56,936]\u001b[0m Trial 0 finished with value: 6.629759669303894 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.629759669303894.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 23:01:56,939]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 124.9100\n",
            "val loss: 14.7215\n",
            "Train epoch: 5, loss: 103.1171\n",
            "val loss: 12.9183\n",
            "Train epoch: 10, loss: 79.8858\n",
            "val loss: 9.2614\n",
            "Train epoch: 15, loss: 72.8238\n",
            "val loss: 8.5394\n",
            "Train epoch: 20, loss: 69.2266\n",
            "val loss: 8.5338\n",
            "Train epoch: 25, loss: 66.7309\n",
            "val loss: 7.9492\n",
            "Train epoch: 30, loss: 64.0995\n",
            "val loss: 7.6767\n",
            "Train epoch: 35, loss: 61.8355\n",
            "val loss: 7.3989\n",
            "Train epoch: 40, loss: 60.3558\n",
            "val loss: 7.2565\n",
            "Train epoch: 45, loss: 58.8696\n",
            "val loss: 7.1922\n",
            "\u001b[32m[I 2023-01-30 23:07:17,581]\u001b[0m Trial 2 finished with value: 6.93555012345314 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.629759669303894.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  6.629759669303894\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.5365\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 23:08:37,069]\u001b[0m A new study created in memory with name: ATA_iso\u001b[0m\n",
            "Train epoch: 0, loss: 126.4461\n",
            "val loss: 14.6857\n",
            "Train epoch: 5, loss: 104.5221\n",
            "val loss: 12.0765\n",
            "Train epoch: 10, loss: 82.6123\n",
            "val loss: 9.6824\n",
            "Train epoch: 15, loss: 73.5337\n",
            "val loss: 8.6593\n",
            "Train epoch: 20, loss: 67.6928\n",
            "val loss: 8.3134\n",
            "/usr/local/lib/python3.8/dist-packages/gpytorch/distributions/multivariate_normal.py:319: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
            "  warnings.warn(\n",
            "Train epoch: 25, loss: 62.9594\n",
            "val loss: 7.7942\n",
            "Train epoch: 30, loss: 59.8361\n",
            "val loss: 7.7605\n",
            "Train epoch: 35, loss: 57.9519\n",
            "val loss: 7.2486\n",
            "Train epoch: 40, loss: 56.4075\n",
            "val loss: 6.9619\n",
            "Train epoch: 45, loss: 54.8110\n",
            "val loss: 7.0783\n",
            "\u001b[32m[I 2023-01-30 23:14:21,449]\u001b[0m Trial 0 finished with value: 6.921832710504532 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.921832710504532.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 23:14:21,452]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 135.2704\n",
            "val loss: 14.9007\n",
            "Train epoch: 5, loss: 101.7697\n",
            "val loss: 10.7094\n",
            "Train epoch: 10, loss: 80.7347\n",
            "val loss: 9.7087\n",
            "Train epoch: 15, loss: 76.3181\n",
            "val loss: 8.7153\n",
            "Train epoch: 20, loss: 72.8620\n",
            "val loss: 8.5068\n",
            "Train epoch: 25, loss: 69.0951\n",
            "val loss: 8.4543\n",
            "Train epoch: 30, loss: 66.7187\n",
            "val loss: 8.0865\n",
            "Train epoch: 35, loss: 64.6549\n",
            "val loss: 7.6332\n",
            "Train epoch: 40, loss: 63.2092\n",
            "val loss: 7.6705\n",
            "Train epoch: 45, loss: 61.6572\n",
            "val loss: 7.5801\n",
            "\u001b[32m[I 2023-01-30 23:19:39,501]\u001b[0m Trial 2 finished with value: 7.338571399450302 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.921832710504532.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  6.921832710504532\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.5530\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 23:21:00,565]\u001b[0m A new study created in memory with name: ATA_no\u001b[0m\n",
            "Train epoch: 0, loss: 126.0139\n",
            "val loss: 14.6376\n",
            "Train epoch: 5, loss: 109.3196\n",
            "val loss: 13.0602\n",
            "Train epoch: 10, loss: 84.9297\n",
            "val loss: 10.0499\n",
            "Train epoch: 15, loss: 77.6180\n",
            "val loss: 9.1214\n",
            "Train epoch: 20, loss: 71.6870\n",
            "val loss: 8.5368\n",
            "Train epoch: 25, loss: 66.9810\n",
            "val loss: 8.1843\n",
            "Train epoch: 30, loss: 63.7671\n",
            "val loss: 7.8864\n",
            "Train epoch: 35, loss: 60.9362\n",
            "val loss: 7.6218\n",
            "Train epoch: 40, loss: 59.6806\n",
            "val loss: 7.5448\n",
            "Train epoch: 45, loss: 58.3001\n",
            "val loss: 7.5120\n",
            "\u001b[32m[I 2023-01-30 23:25:53,774]\u001b[0m Trial 0 finished with value: 7.468951940536499 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 7.468951940536499.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 23:25:53,776]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 134.9653\n",
            "val loss: 14.8477\n",
            "Train epoch: 5, loss: 109.5760\n",
            "val loss: 12.3946\n",
            "Train epoch: 10, loss: 83.5900\n",
            "val loss: 9.6194\n",
            "Train epoch: 15, loss: 78.7005\n",
            "val loss: 9.4027\n",
            "Train epoch: 20, loss: 74.4205\n",
            "val loss: 8.7252\n",
            "Train epoch: 25, loss: 70.7066\n",
            "val loss: 8.4545\n",
            "Train epoch: 30, loss: 68.4753\n",
            "val loss: 8.2835\n",
            "Train epoch: 35, loss: 66.6613\n",
            "val loss: 8.2530\n",
            "Train epoch: 40, loss: 65.2729\n",
            "val loss: 7.9728\n",
            "Train epoch: 45, loss: 64.1019\n",
            "val loss: 7.8825\n",
            "\u001b[32m[I 2023-01-30 23:30:29,282]\u001b[0m Trial 2 finished with value: 7.837698400020599 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 7.468951940536499.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  7.468951940536499\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.6052\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 23:31:47,682]\u001b[0m A new study created in memory with name: ATA_gp\u001b[0m\n",
            "Train epoch: 0, loss: 129.9956\n",
            "val loss: 14.6981\n",
            "Train epoch: 5, loss: 100.6166\n",
            "val loss: 12.6866\n",
            "Train epoch: 10, loss: 83.9716\n",
            "val loss: 10.1200\n",
            "Train epoch: 15, loss: 73.2401\n",
            "val loss: 9.1021\n",
            "Train epoch: 20, loss: 68.4725\n",
            "val loss: 8.6111\n",
            "/usr/local/lib/python3.8/dist-packages/gpytorch/distributions/multivariate_normal.py:319: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
            "  warnings.warn(\n",
            "Train epoch: 25, loss: 64.6817\n",
            "val loss: 8.0192\n",
            "Train epoch: 30, loss: 61.9638\n",
            "val loss: 7.8686\n",
            "Train epoch: 35, loss: 59.4411\n",
            "val loss: 7.5240\n",
            "Train epoch: 40, loss: 56.8363\n",
            "val loss: 7.3644\n",
            "Train epoch: 45, loss: 55.3518\n",
            "val loss: 6.9817\n",
            "\u001b[32m[I 2023-01-30 23:37:30,686]\u001b[0m Trial 0 finished with value: 6.836912423372269 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.836912423372269.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 23:37:30,688]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 130.3804\n",
            "val loss: 14.9149\n",
            "Train epoch: 5, loss: 98.2593\n",
            "val loss: 12.4433\n",
            "Train epoch: 10, loss: 81.4482\n",
            "val loss: 9.6811\n",
            "Train epoch: 15, loss: 75.5430\n",
            "val loss: 8.8660\n",
            "Train epoch: 20, loss: 71.4902\n",
            "val loss: 8.5062\n",
            "Train epoch: 25, loss: 68.3539\n",
            "val loss: 7.9618\n",
            "Train epoch: 30, loss: 65.8473\n",
            "val loss: 7.7512\n",
            "Train epoch: 35, loss: 64.2062\n",
            "val loss: 7.5823\n",
            "Train epoch: 40, loss: 61.8849\n",
            "val loss: 7.6806\n",
            "Train epoch: 45, loss: 60.5632\n",
            "val loss: 7.4870\n",
            "\u001b[32m[I 2023-01-30 23:42:47,113]\u001b[0m Trial 2 finished with value: 7.333299875259399 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.836912423372269.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  6.836912423372269\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.5606\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 23:44:04,991]\u001b[0m A new study created in memory with name: ATA_iso\u001b[0m\n",
            "Train epoch: 0, loss: 124.8410\n",
            "val loss: 14.6409\n",
            "Train epoch: 5, loss: 103.3431\n",
            "val loss: 12.2792\n",
            "Train epoch: 10, loss: 79.9408\n",
            "val loss: 9.4646\n",
            "Train epoch: 15, loss: 71.8458\n",
            "val loss: 8.5021\n",
            "Train epoch: 20, loss: 65.2230\n",
            "val loss: 8.6887\n",
            "/usr/local/lib/python3.8/dist-packages/gpytorch/distributions/multivariate_normal.py:319: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
            "  warnings.warn(\n",
            "Train epoch: 25, loss: 61.2708\n",
            "val loss: 7.8322\n",
            "Train epoch: 30, loss: 58.3382\n",
            "val loss: 7.3244\n",
            "Train epoch: 35, loss: 55.8691\n",
            "val loss: 6.9991\n",
            "Train epoch: 40, loss: 53.5549\n",
            "val loss: 6.9662\n",
            "Train epoch: 45, loss: 52.4776\n",
            "val loss: 6.8012\n",
            "\u001b[32m[I 2023-01-30 23:49:40,867]\u001b[0m Trial 0 finished with value: 6.801213264465332 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.801213264465332.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 23:49:40,870]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 126.6710\n",
            "val loss: 14.9975\n",
            "Train epoch: 5, loss: 103.3196\n",
            "val loss: 12.4227\n",
            "Train epoch: 10, loss: 81.6332\n",
            "val loss: 9.4134\n",
            "Train epoch: 15, loss: 74.8323\n",
            "val loss: 8.5244\n",
            "Train epoch: 20, loss: 70.7964\n",
            "val loss: 8.3585\n",
            "Train epoch: 25, loss: 66.7942\n",
            "val loss: 7.9115\n",
            "Train epoch: 30, loss: 64.0950\n",
            "val loss: 7.7859\n",
            "Train epoch: 35, loss: 62.5426\n",
            "val loss: 7.5444\n",
            "Train epoch: 40, loss: 61.0168\n",
            "val loss: 7.5430\n",
            "Train epoch: 45, loss: 59.7769\n",
            "val loss: 7.4888\n",
            "\u001b[32m[I 2023-01-30 23:54:51,548]\u001b[0m Trial 2 finished with value: 7.383773595094681 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.801213264465332.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  6.801213264465332\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.5750\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 23:56:10,802]\u001b[0m A new study created in memory with name: ATA_no\u001b[0m\n",
            "Train epoch: 0, loss: 124.4557\n",
            "val loss: 14.6028\n",
            "Train epoch: 5, loss: 108.2088\n",
            "val loss: 12.5826\n",
            "Train epoch: 10, loss: 83.4648\n",
            "val loss: 10.4647\n",
            "Train epoch: 15, loss: 73.6248\n",
            "val loss: 10.5548\n",
            "Train epoch: 20, loss: 68.4145\n",
            "val loss: 8.1546\n",
            "Train epoch: 25, loss: 64.5905\n",
            "val loss: 8.3607\n",
            "Train epoch: 30, loss: 61.6969\n",
            "val loss: 7.8030\n",
            "Train epoch: 35, loss: 59.8878\n",
            "val loss: 7.8422\n",
            "Train epoch: 40, loss: 58.0829\n",
            "val loss: 7.4734\n",
            "Train epoch: 45, loss: 56.9803\n",
            "val loss: 7.3549\n",
            "\u001b[32m[I 2023-01-31 00:01:07,429]\u001b[0m Trial 0 finished with value: 7.354917287826538 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 7.354917287826538.\u001b[0m\n",
            "\u001b[32m[I 2023-01-31 00:01:07,432]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 126.2972\n",
            "val loss: 14.9776\n",
            "Train epoch: 5, loss: 108.7736\n",
            "val loss: 12.7923\n",
            "Train epoch: 10, loss: 84.4714\n",
            "val loss: 9.6154\n",
            "Train epoch: 15, loss: 78.0113\n",
            "val loss: 9.0413\n",
            "Train epoch: 20, loss: 74.7725\n",
            "val loss: 8.6634\n",
            "Train epoch: 25, loss: 71.4334\n",
            "val loss: 8.5748\n",
            "Train epoch: 30, loss: 68.9872\n",
            "val loss: 8.6919\n",
            "Train epoch: 35, loss: 67.1773\n",
            "val loss: 8.4901\n",
            "Train epoch: 40, loss: 66.4333\n",
            "val loss: 8.2695\n",
            "Train epoch: 45, loss: 65.2820\n",
            "val loss: 8.3130\n",
            "\u001b[32m[I 2023-01-31 00:05:45,292]\u001b[0m Trial 2 finished with value: 8.230258136987686 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 7.354917287826538.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  7.354917287826538\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.6026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tb\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "from Utils.base_train import batching, ModelData, batch_sampled_data\n",
        "from data_loader import ExperimentConfig\n",
        "from forecast_denoising import Forecast_denoising\n",
        "\n",
        "exp_name = \"solar\"\n",
        "\n",
        "def evaluate(model_name, denoising, gp):\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    pred_len = 24\n",
        "    attn_type = \"ATA\"\n",
        "    n_heads = 8\n",
        "    d_model = [16, 32]\n",
        "    batch_size = 256\n",
        "\n",
        "    config = ExperimentConfig(pred_len, exp_name)\n",
        "\n",
        "    formatter = config.make_data_formatter()\n",
        "    params = formatter.get_experiment_params()\n",
        "    total_time_steps = params['total_time_steps']\n",
        "    num_encoder_steps = params['num_encoder_steps']\n",
        "    column_definition = params[\"column_definition\"]\n",
        "\n",
        "\n",
        "    data_csv_path = \"{}.csv\".format(exp_name)\n",
        "    raw_data = pd.read_csv(data_csv_path)\n",
        "\n",
        "    data = formatter.transform_data(raw_data)\n",
        "    train_max, valid_max = formatter.get_num_samples_for_calibration(num_train=batch_size)\n",
        "    max_samples = (train_max, valid_max)\n",
        "\n",
        "    _, _, test = batch_sampled_data(data, 0.8, max_samples, params['total_time_steps'],\n",
        "                                            params['num_encoder_steps'], pred_len,\n",
        "                                            params[\"column_definition\"],\n",
        "                                            batch_size)\n",
        "\n",
        "    test_enc, test_dec, test_y = next(iter(test))\n",
        "    total_b = len(list(iter(test)))\n",
        "\n",
        "\n",
        "    model_path = \"models_{}_{}\".format(exp_name, pred_len)\n",
        "    model_params = formatter.get_default_model_params()\n",
        "\n",
        "    src_input_size = test_enc.shape[2]\n",
        "    tgt_input_size = test_dec.shape[2]\n",
        "\n",
        "    predictions = np.zeros((3, total_b, test_y.shape[0], test_y.shape[1]))\n",
        "    test_y_tot = torch.zeros((total_b, test_y.shape[0], test_y.shape[1]))\n",
        "    n_batches_test = test_enc.shape[0]\n",
        "\n",
        "\n",
        "    mse = nn.MSELoss()\n",
        "    mae = nn.L1Loss()\n",
        "    stack_size = 1\n",
        "\n",
        "    seed_ls = []\n",
        "    i = -1\n",
        "    for f in os.listdir(model_path):\n",
        "      parts = f.split(\"_\")\n",
        "      \n",
        "      if model_name in f:\n",
        "        i += 1\n",
        "\n",
        "        seed = int(parts[-1])\n",
        "        for d in d_model:\n",
        "              try:\n",
        "                  \n",
        "                  d_k = int(d / n_heads)\n",
        "\n",
        "                  config = src_input_size, tgt_input_size, d, n_heads, d_k, stack_size\n",
        "\n",
        "                  model = Forecast_denoising(\n",
        "                                      pred_len=pred_len,\n",
        "                                      config=config,\n",
        "                                      model_name=model_name,\n",
        "                                      device=device,\n",
        "                                      attn_type=attn_type,\n",
        "                                      seed=seed,\n",
        "                                      denoise=denoising, gp=gp)\n",
        "\n",
        "                  checkpoint = torch.load(os.path.join(\"models_{}_{}\".format(exp_name, pred_len),\n",
        "                                          \"{}_{}\".format(model_name, seed)))\n",
        "                  state_dict = checkpoint['model_state_dict']\n",
        "                  new_state_dict = OrderedDict()\n",
        "\n",
        "                  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "                  model.eval()\n",
        "                  model.to(device)\n",
        "\n",
        "                  j = 0\n",
        "                  for test_enc, test_dec, test_y in test:\n",
        "                      if denoising:\n",
        "                          output, _ = model(test_enc.to(device), test_dec.to(device))\n",
        "                      else:\n",
        "                          output = model(test_enc.to(device), test_dec.to(device))\n",
        "\n",
        "                      predictions[i, j] = output.squeeze(-1).cpu().detach().numpy()\n",
        "                      if i == 0:\n",
        "                          test_y_tot[j] = test_y.squeeze(-1).cpu().detach()\n",
        "                      j += 1\n",
        "\n",
        "              except RuntimeError as e:\n",
        "                    pass\n",
        "        \n",
        "\n",
        "    predictions = torch.from_numpy(np.mean(predictions, axis=0))\n",
        "\n",
        "    results = torch.zeros(2, pred_len)\n",
        "    normaliser = test_y_tot.abs().mean()\n",
        "\n",
        "    mse_loss = mse(predictions, test_y_tot).item() / normaliser\n",
        "    mae_loss = mae(predictions, test_y_tot).item() / normaliser\n",
        "\n",
        "    print(\"{}: MSE: {:.3f}\".format(model_name, mse_loss))\n",
        "    print(\"{}: MAE: {:.3f}\".format(model_name, mae_loss))"
      ],
      "metadata": {
        "id": "TovjOwG1HFyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(\"ATA_gp\", True, True)\n",
        "evaluate(\"ATA_iso\", True, False)\n",
        "evaluate(\"ATA_no\", False, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-weC5WVIZEme",
        "outputId": "6a21d2dc-e679-48cc-f3cb-ec983784332c"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using root folder /content/Corruption-resilient-Forecasting-Models/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "ATA_gp: MSE: 0.493\n",
            "ATA_gp: MAE: 0.584\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "ATA_iso: MSE: 0.513\n",
            "ATA_iso: MAE: 0.591\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "ATA_no: MSE: 0.570\n",
            "ATA_no: MAE: 0.636\n"
          ]
        }
      ]
    }
  ]
}